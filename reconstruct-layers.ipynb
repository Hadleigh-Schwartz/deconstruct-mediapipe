{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "234af157-8336-40aa-9536-948d3f6a578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blendshape_info import BLENDSHAPE_MODEL_LANDMARKS_SUBSET, BLENDSHAPE_NAMES, parse_prototxt\n",
    "\n",
    "path = 'face_blendshapes_in_landmarks.prototxt'\n",
    "test_lmks_in = parse_prototxt(path)[:, :2]\n",
    "path = 'face_blendshapes_out.prototxt'\n",
    "test_blshps_out = parse_prototxt(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "36dcf221-5e36-4a54-a7c3-89cfd52dff3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 1, 97)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import helper\n",
    "import onnxruntime as rt\n",
    "\n",
    "onnx_path = './face_blendshapes.onnx'\n",
    "modified_model = onnx.load(onnx_path)\n",
    "intermediate_layer_value_info = helper.ValueInfoProto()\n",
    "\n",
    "v = ['concat']\n",
    "v = ['model_1/tf.__operators__.getitem_2/strided_slice3']\n",
    "# v = ['model_1/tf.compat.v1.norm/norm/mul']\n",
    "# [(i, n) for i, n in enumerate(modified_model.graph.node) if v[0] in str(n)]\n",
    "# v = ['model_1/tf.compat.v1.norm/norm/Sqrt']\n",
    "v = ['model_1/GhumMarkerPoserMlpMixerGeneral/conv2d/BiasAdd;model_1/GhumMarkerPoserMlpMixerGeneral/conv2d/Conv2D;model_1/GhumMarkerPoserMlpMixerGeneral/conv2d/BiasAdd/ReadVariableOp']\n",
    "v = ['model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/input_tokens_embedding/BiasAdd;model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_3/mlp_channel_mixing/Mlp_2/Conv2D;model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/input_tokens_embedding/Conv2D;model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/input_tokens_embedding/BiasAdd/ReadVariableOp']\n",
    "\n",
    "v = ['model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/AddExtraTokens/concat']\n",
    "v = ['model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/layer_norm1/moments/SquaredDifference__43:0']\n",
    "v = ['model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/Relu;model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/BiasAdd;model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_3/mlp_token_mixing/Mlp_1/Conv2D;model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/Conv2D;model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/BiasAdd/ReadVariableOp__45:0']\n",
    "# v = ['model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/layer_norm1/batchnorm/add_1']\n",
    "v = '''\n",
    "name: model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_3/residual_channels/add\n",
    "'''\n",
    "if not isinstance(v, list):\n",
    "    v = [(v.split(\"name: \")[1] if len(v.split(\"name: \")) > 1 else v).strip()]\n",
    "\n",
    "modified_model_path = '/tmp/modified.onnx'\n",
    "intermediate_layer_value_info = helper.ValueInfoProto()\n",
    "intermediate_layer_value_info.name = v[0]\n",
    "modified_model.graph.output.append(intermediate_layer_value_info)\n",
    "onnx.save(modified_model, modified_model_path)\n",
    "\n",
    "modified_model = onnx.load(modified_model_path)\n",
    "\n",
    "providers = ['CPUExecutionProvider']\n",
    "m = rt.InferenceSession(modified_model_path, providers=providers)\n",
    "input_names = [n.name for n in modified_model.graph.input]\n",
    "\n",
    "xx = test_lmks_in[None, BLENDSHAPE_MODEL_LANDMARKS_SUBSET ]\n",
    "outs = m.run(v,  {input_names[0]: xx})\n",
    "outs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2e484654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conv graph nodes:\n",
    "# ' '.join([str(i) for i in range(len(modified_model.graph.node)) if 'Conv' in str(modified_model.graph.node[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f5c6bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'extra_token', 'mlpmixer_blocks.0.norm1.weight', 'mlpmixer_blocks.0.mlp_token_mixing.0.weight', 'mlpmixer_blocks.0.mlp_token_mixing.0.bias', 'mlpmixer_blocks.0.mlp_token_mixing.3.weight', 'mlpmixer_blocks.0.mlp_token_mixing.3.bias', 'mlpmixer_blocks.0.norm2.weight', 'mlpmixer_blocks.0.mlp_channel_mixing.0.weight', 'mlpmixer_blocks.0.mlp_channel_mixing.0.bias', 'mlpmixer_blocks.0.mlp_channel_mixing.3.weight', 'mlpmixer_blocks.0.mlp_channel_mixing.3.bias', 'mlpmixer_blocks.1.norm1.weight', 'mlpmixer_blocks.1.mlp_token_mixing.0.weight', 'mlpmixer_blocks.1.mlp_token_mixing.0.bias', 'mlpmixer_blocks.1.mlp_token_mixing.3.weight', 'mlpmixer_blocks.1.mlp_token_mixing.3.bias', 'mlpmixer_blocks.1.norm2.weight', 'mlpmixer_blocks.1.mlp_channel_mixing.0.weight', 'mlpmixer_blocks.1.mlp_channel_mixing.0.bias', 'mlpmixer_blocks.1.mlp_channel_mixing.3.weight', 'mlpmixer_blocks.1.mlp_channel_mixing.3.bias', 'mlpmixer_blocks.2.norm1.weight', 'mlpmixer_blocks.2.mlp_token_mixing.0.weight', 'mlpmixer_blocks.2.mlp_token_mixing.0.bias', 'mlpmixer_blocks.2.mlp_token_mixing.3.weight', 'mlpmixer_blocks.2.mlp_token_mixing.3.bias', 'mlpmixer_blocks.2.norm2.weight', 'mlpmixer_blocks.2.mlp_channel_mixing.0.weight', 'mlpmixer_blocks.2.mlp_channel_mixing.0.bias', 'mlpmixer_blocks.2.mlp_channel_mixing.3.weight', 'mlpmixer_blocks.2.mlp_channel_mixing.3.bias', 'mlpmixer_blocks.3.norm1.weight', 'mlpmixer_blocks.3.mlp_token_mixing.0.weight', 'mlpmixer_blocks.3.mlp_token_mixing.0.bias', 'mlpmixer_blocks.3.mlp_token_mixing.3.weight', 'mlpmixer_blocks.3.mlp_token_mixing.3.bias', 'mlpmixer_blocks.3.norm2.weight', 'mlpmixer_blocks.3.mlp_channel_mixing.0.weight', 'mlpmixer_blocks.3.mlp_channel_mixing.0.bias', 'mlpmixer_blocks.3.mlp_channel_mixing.3.weight', 'mlpmixer_blocks.3.mlp_channel_mixing.3.bias'])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from onnx.numpy_helper import to_array\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_model_weight(onnx_model, name):\n",
    "    for _ in onnx_model.graph.initializer:\n",
    "        if _.name == name:\n",
    "            break\n",
    "    return _\n",
    "\n",
    "def conv_node_to_w_b(onnx_model, node):\n",
    "    w_node = node.input[1]\n",
    "    w = to_array(get_model_weight(onnx_model, w_node))\n",
    "    assert len(w.shape) == 4, w.shape\n",
    "    b_node = node.input[2]\n",
    "    b = to_array(get_model_weight(onnx_model, b_node))\n",
    "    assert len(b.shape) == 1, b.shape\n",
    "    w, b = [torch.from_numpy(_).float() for _ in [w, b]]\n",
    "    return w, b\n",
    "\n",
    "def get_layernorm_weight(onnx_model, mixer_block_idx, norm_idx):\n",
    "    search_str = f'model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_{mixer_block_idx}/layer_norm{norm_idx}/batchnorm/mul'\n",
    "    fold_op_name = [n for n in onnx_model.graph.node if search_str == n.output[0]][0].input[1]\n",
    "    return get_model_weight(onnx_model, fold_op_name)\n",
    "\n",
    "def get_conv_layer_weight_bias(onnx_model, mixer_block_idx, is_token_mixer, mlp_idx):\n",
    "    # print(mixer_block_idx, is_token_mixer, mlp_idx)\n",
    "    assert mlp_idx in (1, 2)\n",
    "    assert mixer_block_idx in (0, 1, 2, 3)\n",
    "    search_str = '''\n",
    "    model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/Relu;\n",
    "    model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/BiasAdd;\n",
    "    model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_3/mlp_token_mixing/Mlp_1/Conv2D;\n",
    "    model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/Conv2D;\n",
    "    model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/BiasAdd/ReadVariableOp\n",
    "    '''\n",
    "    if mixer_block_idx == 3:\n",
    "        search_str = '''\n",
    "        model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/Relu;\n",
    "        model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/BiasAdd;\n",
    "        model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/Conv2D;\n",
    "        model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/BiasAdd/ReadVariableOp\n",
    "        '''\n",
    "\n",
    "    \n",
    "    search_str = search_str.replace(\"\\n\", \"\").replace(\" \", \"\").strip()\n",
    "\n",
    "    search_str = search_str.replace(\"MixerBlock_0\", f'MixerBlock_{mixer_block_idx}')\n",
    "    if mlp_idx == 2:\n",
    "        replace_str = \"model_1/GhumMarkerPoserMlpMixerGeneral/MLPMixer/MixerBlock_0/mlp_token_mixing/Mlp_1/Relu;\"\n",
    "        replace_str = replace_str.replace(\"MixerBlock_0\", f'MixerBlock_{mixer_block_idx}')\n",
    "        search_str = search_str.replace(replace_str, \"\")\n",
    "    search_str = search_str.replace(\"Mlp_1\", f'Mlp_{mlp_idx}')\n",
    "    if not is_token_mixer:\n",
    "        search_str = search_str.replace(\"mlp_token_mixing\", \"mlp_channel_mixing\")\n",
    "    # print(search_str)\n",
    "    ii, node = [(i, n) for i, n in enumerate(onnx_model.graph.node) if search_str == n.output[0]][0]\n",
    "    # print(ii)\n",
    "    w, b = conv_node_to_w_b(onnx_model, node)\n",
    "    mlpname = 'mlp_token_mixing' if is_token_mixer else 'mlp_channel_mixing'\n",
    "    idx = 0 if mlp_idx == 1 else 3\n",
    "    # print(w.shape, b.shape)\n",
    "    # print(w[:2, :2, 0, 0], b[:2]    )\n",
    "    return {f\"mlpmixer_blocks.{mixer_block_idx}.{mlpname}.{idx}.weight\": w, f\"mlpmixer_blocks.{mixer_block_idx}.{mlpname}.{idx}.bias\": b}\n",
    "\n",
    "\n",
    "def get_state_dict_mlp_mixer_layer(onnx_model, mixer_block_idx):\n",
    "    state_dict = {}\n",
    "    norm1_weight = get_layernorm_weight(onnx_model, mixer_block_idx, 1)\n",
    "    state_dict.update({f\"mlpmixer_blocks.{mixer_block_idx}.norm1.weight\": torch.from_numpy(to_array(norm1_weight)).float()})\n",
    "    state_dict.update(get_conv_layer_weight_bias(onnx_model, mixer_block_idx, True, 1))\n",
    "    state_dict.update(get_conv_layer_weight_bias(onnx_model, mixer_block_idx, True, 2))\n",
    "    norm2_weight = get_layernorm_weight(modified_model, mixer_block_idx, 2)\n",
    "    state_dict.update({f\"mlpmixer_blocks.{mixer_block_idx}.norm2.weight\": torch.from_numpy(to_array(norm2_weight)).float()})\n",
    "    state_dict.update(get_conv_layer_weight_bias(onnx_model, mixer_block_idx, False, 1))\n",
    "    state_dict.update(get_conv_layer_weight_bias(onnx_model, mixer_block_idx, False, 2))\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def get_state_dict(onnx_model):\n",
    "    w, b = conv_node_to_w_b(onnx_model, onnx_model.graph.node[11])\n",
    "    state_dict = {\"conv1.weight\": w, \"conv1.bias\": b}\n",
    "    w, b = conv_node_to_w_b(onnx_model, onnx_model.graph.node[13])\n",
    "    state_dict.update({\"conv2.weight\": w, \"conv2.bias\": b})\n",
    "    extra_token = get_model_weight(onnx_model, 'const_fold_opt__350')\n",
    "    state_dict.update({\"extra_token\": torch.from_numpy(to_array(extra_token)).float()})\n",
    "\n",
    "    # MLP Mixer layers\n",
    "    # layer 0\n",
    "    state_dict.update(get_state_dict_mlp_mixer_layer(onnx_model, 0))\n",
    "    # layer 1\n",
    "    state_dict.update(get_state_dict_mlp_mixer_layer(onnx_model, 1))\n",
    "    # layer 2\n",
    "    state_dict.update(get_state_dict_mlp_mixer_layer(onnx_model, 2))\n",
    "    # layer 3\n",
    "    state_dict.update(get_state_dict_mlp_mixer_layer(onnx_model, 3))\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "get_state_dict(modified_model).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "6ada3877-b9a3-421c-ac48-0930c8b2a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['extra_token', 'conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'mlpmixer_blocks.0.mlp_token_mixing.0.weight', 'mlpmixer_blocks.0.mlp_token_mixing.0.bias', 'mlpmixer_blocks.0.mlp_token_mixing.3.weight', 'mlpmixer_blocks.0.mlp_token_mixing.3.bias', 'mlpmixer_blocks.0.mlp_channel_mixing.0.weight', 'mlpmixer_blocks.0.mlp_channel_mixing.0.bias', 'mlpmixer_blocks.0.mlp_channel_mixing.3.weight', 'mlpmixer_blocks.0.mlp_channel_mixing.3.bias', 'mlpmixer_blocks.0.norm1.weight', 'mlpmixer_blocks.0.norm2.weight', 'mlpmixer_blocks.1.mlp_token_mixing.0.weight', 'mlpmixer_blocks.1.mlp_token_mixing.0.bias', 'mlpmixer_blocks.1.mlp_token_mixing.3.weight', 'mlpmixer_blocks.1.mlp_token_mixing.3.bias', 'mlpmixer_blocks.1.mlp_channel_mixing.0.weight', 'mlpmixer_blocks.1.mlp_channel_mixing.0.bias', 'mlpmixer_blocks.1.mlp_channel_mixing.3.weight', 'mlpmixer_blocks.1.mlp_channel_mixing.3.bias', 'mlpmixer_blocks.1.norm1.weight', 'mlpmixer_blocks.1.norm2.weight', 'mlpmixer_blocks.2.mlp_token_mixing.0.weight', 'mlpmixer_blocks.2.mlp_token_mixing.0.bias', 'mlpmixer_blocks.2.mlp_token_mixing.3.weight', 'mlpmixer_blocks.2.mlp_token_mixing.3.bias', 'mlpmixer_blocks.2.mlp_channel_mixing.0.weight', 'mlpmixer_blocks.2.mlp_channel_mixing.0.bias', 'mlpmixer_blocks.2.mlp_channel_mixing.3.weight', 'mlpmixer_blocks.2.mlp_channel_mixing.3.bias', 'mlpmixer_blocks.2.norm1.weight', 'mlpmixer_blocks.2.norm2.weight', 'mlpmixer_blocks.3.mlp_token_mixing.0.weight', 'mlpmixer_blocks.3.mlp_token_mixing.0.bias', 'mlpmixer_blocks.3.mlp_token_mixing.3.weight', 'mlpmixer_blocks.3.mlp_token_mixing.3.bias', 'mlpmixer_blocks.3.mlp_channel_mixing.0.weight', 'mlpmixer_blocks.3.mlp_channel_mixing.0.bias', 'mlpmixer_blocks.3.mlp_channel_mixing.3.weight', 'mlpmixer_blocks.3.mlp_channel_mixing.3.bias', 'mlpmixer_blocks.3.norm1.weight', 'mlpmixer_blocks.3.norm2.weight'])\n",
      "torch: (1, 64, 1, 97)\n",
      "onnx : (1, 64, 1, 97)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1457672e-05"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLPMixerLayer(nn.Module):\n",
    "    def __init__(self, in_dim, num_patches, hidden_units_mlp1, hidden_units_mlp2, dropout_rate=0.0, eps1=0.0000010132789611816406, eps2=0.0000010132789611816406):\n",
    "        super().__init__()\n",
    "        self.mlp_token_mixing = nn.Sequential(\n",
    "            nn.Conv2d(num_patches, hidden_units_mlp1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(hidden_units_mlp1, num_patches, 1)\n",
    "        )\n",
    "        self.mlp_channel_mixing = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, hidden_units_mlp2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(hidden_units_mlp2, in_dim, 1)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(in_dim, bias=False, elementwise_affine=True, eps=eps1)\n",
    "        self.norm2 = nn.LayerNorm(in_dim, bias=False, elementwise_affine=True, eps=eps2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.norm1(x)\n",
    "        mlp1_outputs = self.mlp_token_mixing(x_1)\n",
    "        x = x + mlp1_outputs\n",
    "        x_2 = self.norm2(x)\n",
    "        mlp2_outputs = self.mlp_channel_mixing(x_2.permute(0, 3, 2, 1))\n",
    "        x = x + mlp2_outputs.permute(0, 3, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    def __init__(self, in_dim=64, num_patches=97, hidden_units_mlp1=384, hidden_units_mlp2=256, num_blocks=4, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(146, 96, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(2, 64, kernel_size=1)\n",
    "        self.extra_token = nn.Parameter(torch.randn(1, 64, 1, 1), requires_grad=True)\n",
    "        self.mlpmixer_blocks = nn.Sequential(\n",
    "            *[MLPMixerLayer(in_dim, num_patches, hidden_units_mlp1, hidden_units_mlp2, dropout_rate) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x - x.mean(1, keepdim=True)\n",
    "        x = x / x.norm(dim=2, keepdim=True).mean(1, keepdim=True)\n",
    "        x = x.unsqueeze(-2) * 0.5\n",
    "        x = self.conv1(x)\n",
    "        x = x.permute(0, 3, 2, 1)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.cat([self.extra_token, x], dim=3)\n",
    "        x = x.permute(0, 3, 2, 1)\n",
    "        x = self.mlpmixer_blocks(x)\n",
    "        return x.permute(0, 3, 2, 1)\n",
    "\n",
    "m = MLPMixer()\n",
    "print(m.state_dict().keys())\n",
    "m.load_state_dict(get_state_dict(modified_model))\n",
    "xx_tch = torch.from_numpy(xx)\n",
    "tch_out = m(xx_tch).detach().numpy()\n",
    "onnx_out = outs[0]\n",
    "# print(tch_out[:5])\n",
    "# print(onnx_out[:5])\n",
    "print(\"torch:\", tch_out.shape)\n",
    "print(\"onnx :\", onnx_out.shape)\n",
    "np.abs(tch_out - onnx_out).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2dec119c-1c57-4b62-a4d1-b95a45c5a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMixerLayer(nn.Module):\n",
    "    def __init__(self, num_patches, hidden_units, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(num_patches, num_patches),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(num_patches, num_patches)\n",
    "        )\n",
    "\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(num_patches, num_patches),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(num_patches, hidden_units)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(num_patches)\n",
    "        self.norm2 = nn.LayerNorm(hidden_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization\n",
    "        x = self.norm1(x)\n",
    "        return x\n",
    "\n",
    "        # Transpose inputs from [batch_size, num_patches, hidden_units] to [batch_size, hidden_units, num_patches]\n",
    "        x_channels = x.transpose(1, 2)\n",
    "\n",
    "        # Apply mlp1 on each channel independently\n",
    "        mlp1_outputs = self.mlp1(x_channels)\n",
    "\n",
    "        # Transpose mlp1_outputs from [batch_size, hidden_units, num_patches] to [batch_size, num_patches, hidden_units]\n",
    "        mlp1_outputs = mlp1_outputs.transpose(1, 2)\n",
    "\n",
    "        # Add skip connection\n",
    "        x = mlp1_outputs + x\n",
    "\n",
    "        # Apply layer normalization\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # Apply mlp2 on each patch independently\n",
    "        mlp2_outputs = self.mlp2(x)\n",
    "\n",
    "        # Add skip connection\n",
    "        x = mlp2_outputs + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c18876ed-e9e2-49fc-bbee-716fc0e8f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    class MLPMixerLayer(layers.Layer):\n",
    "        def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "\n",
    "            self.mlp1 = keras.Sequential(\n",
    "                [\n",
    "                    layers.Dense(units=num_patches, activation=\"gelu\"),\n",
    "                    layers.Dense(units=num_patches),\n",
    "                    layers.Dropout(rate=dropout_rate),\n",
    "                ]\n",
    "            )\n",
    "            self.mlp2 = keras.Sequential(\n",
    "                [\n",
    "                    layers.Dense(units=num_patches, activation=\"gelu\"),\n",
    "                    layers.Dense(units=hidden_units),\n",
    "                    layers.Dropout(rate=dropout_rate),\n",
    "                ]\n",
    "            )\n",
    "            self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            return super().build(input_shape)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            # Apply layer normalization.\n",
    "            x = self.normalize(inputs)\n",
    "            # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
    "            x_channels = keras.ops.transpose(x, axes=(0, 2, 1))\n",
    "            # Apply mlp1 on each channel independently.\n",
    "            mlp1_outputs = self.mlp1(x_channels)\n",
    "            # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
    "            mlp1_outputs = keras.ops.transpose(mlp1_outputs, axes=(0, 2, 1))\n",
    "            # Add skip connection.\n",
    "            x = mlp1_outputs + inputs\n",
    "            # Apply layer normalization.\n",
    "            x_patches = self.normalize(x)\n",
    "            # Apply mlp2 on each patch independtenly.\n",
    "            mlp2_outputs = self.mlp2(x_patches)\n",
    "            # Add skip connection.\n",
    "            x = x + mlp2_outputs\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee49b1-c1af-4da8-af06-9025ba7f7e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
